{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZTS1ZvrgxYl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision numpy matplotlib pycocotools"
      ],
      "metadata": {
        "id": "uJxz4gzAw0fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision"
      ],
      "metadata": {
        "id": "0qCWUwIRxIoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torchvision.transforms import functional as F\n",
        "import torch"
      ],
      "metadata": {
        "id": "GUyErGtIxIy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "5j6WcUJBNGk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_npy_image(npy_path):\n",
        "    data = np.load(npy_path)  # Load .npy file\n",
        "    rgb = data[..., :3]  # Extract RGB channels\n",
        "    rgb_tensor = torch.from_numpy(rgb).permute(2, 0, 1) / 255.0  # Normalize to [0, 1]\n",
        "    return rgb_tensor"
      ],
      "metadata": {
        "id": "LtUUB09m0z37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from pycocotools import mask as coco_mask\n",
        "import numpy as np\n",
        "\n",
        "class ThermalBridgeDataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, transforms=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            self.coco_data = json.load(f)\n",
        "\n",
        "        self.images = {img['id']: img['file_name'] for img in self.coco_data['images']}\n",
        "        '''self.annotations = {}\n",
        "        for ann in self.coco_data['annotations']:\n",
        "            img_id = ann['image_id']\n",
        "            self.annotations.setdefault(img_id, []).append(ann)'''\n",
        "\n",
        "        self.annotations = {}\n",
        "        for ann in self.coco_data['annotations']:\n",
        "            #print(ann)\n",
        "            img_id = ann['image_id']\n",
        "            #print(img_id)\n",
        "            if img_id not in self.annotations:\n",
        "                self.annotations[img_id] = []\n",
        "            self.annotations[img_id].append(ann)\n",
        "\n",
        "        self.valid_images = []\n",
        "        for img_id, file_name in self.images.items():\n",
        "            img_path = os.path.join(self.root_dir, file_name)\n",
        "            if os.path.exists(img_path):\n",
        "                self.valid_images.append((img_id, img_path))\n",
        "            else:\n",
        "                print(f\"Skipping missing file: {img_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, img_path = self.valid_images[idx]\n",
        "        image = load_npy_image(img_path)  # You mentioned this in your original code\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "\n",
        "        for ann in self.annotations[img_id]:\n",
        "            # Bounding box\n",
        "            x_min, y_min, width, height = ann['bbox']\n",
        "            x_max, y_max = x_min + width, y_min + height\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "            labels.append(1)  # Thermal bridge = class 1\n",
        "\n",
        "            # Segmentation mask\n",
        "            segmentation = ann.get('segmentation')\n",
        "            if segmentation is not None:\n",
        "                if isinstance(segmentation, list):\n",
        "                    # Polygon\n",
        "                    rles = coco_mask.frPyObjects(segmentation, image.shape[1], image.shape[2])\n",
        "                    rle = coco_mask.merge(rles)\n",
        "                else:\n",
        "                    # RLE directly\n",
        "                    rle = segmentation\n",
        "\n",
        "                binary_mask = coco_mask.decode(rle)\n",
        "                if binary_mask.ndim == 3:\n",
        "                    binary_mask = binary_mask[:, :, 0]  # take first channel\n",
        "                masks.append(torch.tensor(binary_mask, dtype=torch.uint8))\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        masks = torch.stack(masks, dim=0) if masks else torch.zeros((0, image.shape[1], image.shape[2]), dtype=torch.uint8)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"masks\": masks\n",
        "\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "F-YpcsDU1OpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "EF3u72kfNIqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "\n",
        "# Load pretrained weights\n",
        "weights = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = maskrcnn_resnet50_fpn_v2(weights=weights)\n",
        "\n",
        "# Modify the heads for your custom dataset (2 classes = 1 object class + background)\n",
        "num_classes = 2\n",
        "\n",
        "# Replace box predictor\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Replace mask predictor\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "hidden_layer = 256\n",
        "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
        "    in_features_mask, hidden_layer, num_classes\n",
        ")\n"
      ],
      "metadata": {
        "id": "bCyXi5cy2gdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Paths\n",
        "train_images_dir = \"/content/drive/MyDrive/TBBR/train/images/\"\n",
        "train_annotations_file = \"/content/drive/MyDrive/TBBR/train/Flug1_100-104Media_coco.json\"\n",
        "#test_images_dir = \"test/images/\"\n",
        "#test_annotations_file = \"test/Flug1_105Media_coco.json\"\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "train_dataset = ThermalBridgeDataset(train_images_dir, train_annotations_file)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Define optimizer\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-4)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 40\n",
        "\n",
        "\n",
        "all_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "        steps += 1\n",
        "\n",
        "    avg_loss = epoch_loss / steps\n",
        "    all_losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch + 1}, Avg Loss: {avg_loss:.4f}\")\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "plt.plot(range(1, num_epochs + 1), all_losses, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I7hk3bsJch2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing and Prediction"
      ],
      "metadata": {
        "id": "76Cv2SvINMgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "xsuYuOSfgSa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"thermal_bridge_maskedrcnn_v2.pth\")"
      ],
      "metadata": {
        "id": "bPYVIG44h34i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "test_images_dir = \"/content/drive/MyDrive/TBBR/test/images/\"\n",
        "test_annotations_file = \"/content/drive/MyDrive/TBBR/test/Flug1_105Media_coco.json\"\n",
        "\n",
        "test_dataset = ThermalBridgeDataset(test_images_dir, test_annotations_file)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n"
      ],
      "metadata": {
        "id": "0yO-Vylexbkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "#model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thermal bridge models/thermal_bridge_maskedrcnn_v2.pth\"))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "yNXcFXnYhiU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict for induvidual image"
      ],
      "metadata": {
        "id": "5JtLD2TAkMzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Choose index of the image in your dataset\n",
        "index = 15 # Change this to test other images\n",
        "\n",
        "# Get image (no need for the target during inference)\n",
        "image, _ = test_dataset[index]  # Assuming your dataset object is named `test_dataset`\n",
        "\n",
        "# Add batch dimension and move to device\n",
        "image_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor)[0]\n"
      ],
      "metadata": {
        "id": "ZJhv1BnQmpfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert tensor to NumPy array for display\n",
        "img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "plt.imshow(img_np, cmap='gray')  # Use 'gray' if single-channel\n",
        "\n",
        "# Draw bounding boxes\n",
        "for box in output['boxes'].cpu().numpy():\n",
        "    x1, y1, x2, y2 = box.astype(int)\n",
        "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                      fill=False, edgecolor='red', linewidth=2))\n",
        "\n",
        "plt.title(\"Detected Boxes\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iOmdd8HGml9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def display_comparison(model, dataset, index, device, score_threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    # Load image and ground truth\n",
        "    image, target = dataset[index]\n",
        "    image_tensor = image.unsqueeze(0).to(device)\n",
        "    target = {k: v.to(device) for k, v in target.items()}\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)[0]\n",
        "\n",
        "    # Convert image to NumPy\n",
        "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Prepare figure\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # 1. Original Image\n",
        "    axs[0].imshow(img_np, cmap='gray')\n",
        "    axs[0].set_title(\"Original Image\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    # 2. Ground Truth\n",
        "    axs[1].imshow(img_np, cmap='gray')\n",
        "    for box in target['boxes'].cpu().numpy():\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        axs[1].add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                           linewidth=2, edgecolor='green', facecolor='none'))\n",
        "    for mask in target['masks'].cpu().numpy():\n",
        "        axs[1].imshow(np.ma.masked_where(mask == 0, mask), alpha=0.5, cmap='Greens')\n",
        "    axs[1].set_title(\"Ground Truth\")\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    # 3. Predictions\n",
        "    axs[2].imshow(img_np, cmap='gray')\n",
        "    for box, score in zip(output['boxes'].cpu().numpy(), output['scores'].cpu().numpy()):\n",
        "        if score >= score_threshold:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            axs[2].add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                               linewidth=2, edgecolor='red', facecolor='none'))\n",
        "            axs[2].text(x1, y1 - 5, f\"{score:.2f}\", color='red', fontsize=8)\n",
        "    for i, mask in enumerate(output['masks'].cpu().numpy()):\n",
        "        if output['scores'][i] >= score_threshold:\n",
        "            axs[2].imshow(np.ma.masked_where(mask[0] < 0.5, mask[0]), alpha=0.5, cmap='Reds')\n",
        "    axs[2].set_title(\"Predictions\")\n",
        "    axs[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "s-Zv6hAweO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume model and dataset are ready\n",
        "display_comparison(model, test_dataset, index=67, device=device, score_threshold=0.1)\n"
      ],
      "metadata": {
        "id": "nEDVZbPseUb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def display_overlay(model, dataset, index, device, score_threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    # Load image and ground truth\n",
        "    image, target = dataset[index]\n",
        "    image_tensor = image.unsqueeze(0).to(device)\n",
        "    target = {k: v.to(device) for k, v in target.items()}\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)[0]\n",
        "\n",
        "    # Convert image to NumPy\n",
        "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Prepare figure\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img_np)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # Plot ground truth boxes and masks in green\n",
        "    for box in target['boxes'].cpu().numpy():\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        ax.add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                       linewidth=2, edgecolor='green', facecolor='none'))\n",
        "    for mask in target['masks'].cpu().numpy():\n",
        "        plt.imshow(np.ma.masked_where(mask == 0, mask), alpha=0.3, cmap='Greens')\n",
        "\n",
        "    # Plot predicted boxes and masks in red\n",
        "    for box, score in zip(output['boxes'].cpu().numpy(), output['scores'].cpu().numpy()):\n",
        "        if score >= score_threshold:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            ax.add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                           linewidth=2, edgecolor='red', facecolor='none'))\n",
        "            ax.text(x1, y1 - 5, f\"{score:.2f}\", color='red', fontsize=8)\n",
        "    for i, mask in enumerate(output['masks'].cpu().numpy()):\n",
        "        if output['scores'][i] >= score_threshold:\n",
        "            plt.imshow(np.ma.masked_where(mask[0] < 0.5, mask[0]), alpha=0.3, cmap='Reds')\n",
        "\n",
        "    plt.title(\"Overlay: Ground Truth (Green) vs Predictions (Red)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "smGYgj_s7X0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_overlay(model, test_dataset, index=25, device=device, score_threshold=0.1)"
      ],
      "metadata": {
        "id": "-1rdh7G67bYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops import box_iou\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, dataset, device, iou_threshold=0.5, score_threshold=0.4):\n",
        "    model.eval()\n",
        "    aps, all_precisions, all_recalls, all_accuracies = [], [], [], []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        image, target = dataset[i]\n",
        "        image_tensor = image.to(device).unsqueeze(0)\n",
        "        target = {k: v.to(device) for k, v in target.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image_tensor)[0]\n",
        "\n",
        "        pred_boxes = output['boxes'][output['scores'] > score_threshold]\n",
        "        gt_boxes = target['boxes']\n",
        "\n",
        "        # If no predictions or GTs\n",
        "        if len(pred_boxes) == 0 and len(gt_boxes) == 0:\n",
        "            precision = recall = accuracy = ap = 1.0\n",
        "            tp = fp = fn = 0\n",
        "        elif len(pred_boxes) == 0:\n",
        "            tp = 0\n",
        "            fp = 0\n",
        "            fn = len(gt_boxes)\n",
        "            precision = recall = accuracy = ap = 0.0\n",
        "        elif len(gt_boxes) == 0:\n",
        "            tp = 0\n",
        "            fp = len(pred_boxes)\n",
        "            fn = 0\n",
        "            precision = recall = accuracy = ap = 0.0\n",
        "        else:\n",
        "            ious = box_iou(pred_boxes, gt_boxes)\n",
        "            matched = (ious > iou_threshold).any(dim=1).cpu().numpy()\n",
        "            tp = matched.sum()\n",
        "            fp = len(pred_boxes) - tp\n",
        "            fn = len(gt_boxes) - tp\n",
        "\n",
        "            precision = tp / (tp + fp + 1e-6)\n",
        "            recall = tp / (tp + fn + 1e-6)\n",
        "            accuracy = tp / (tp + fp + fn + 1e-6)\n",
        "            ap = precision * recall  # crude AP approximation\n",
        "\n",
        "        all_precisions.append(precision)\n",
        "        all_recalls.append(recall)\n",
        "        all_accuracies.append(accuracy)\n",
        "        aps.append(ap)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n==== Overall Metrics ====\")\n",
        "    print(f\"Mean Precision: {np.mean(all_precisions):.4f}\")\n",
        "    print(f\"Mean Recall:    {np.mean(all_recalls):.4f}\")\n",
        "    print(f\"Mean Accuracy:  {np.mean(all_accuracies):.4f}\")\n",
        "    print(f\"mAP (approx):   {np.mean(aps):.4f}\")\n"
      ],
      "metadata": {
        "id": "xYhpHQOl7eRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_dataset, device=device)"
      ],
      "metadata": {
        "id": "kju56XZR7ilc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}