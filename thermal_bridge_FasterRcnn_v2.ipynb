{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZTS1ZvrgxYl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision numpy matplotlib pycocotools"
      ],
      "metadata": {
        "id": "uJxz4gzAw0fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torchvision.transforms import functional as F\n",
        "import torch"
      ],
      "metadata": {
        "id": "0qCWUwIRxIoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "EjiAqLIZNRky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dont run\n",
        "def load_npy_image(npy_path):\n",
        "    data = np.load(npy_path)  # Load .npy file\n",
        "    rgb = data[..., :3]  # Extract RGB channels\n",
        "    rgb_tensor = torch.from_numpy(rgb).permute(2, 0, 1) / 255.0  # Normalize to [0, 1]\n",
        "    return rgb_tensor"
      ],
      "metadata": {
        "id": "LtUUB09m0z37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class ThermalBridgeDataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, transforms=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "        with open(annotation_file, 'r') as f:\n",
        "\n",
        "            self.coco_data = json.load(f)\n",
        "\n",
        "        self.images = {img['id']: img['file_name'] for img in self.coco_data['images']}\n",
        "        #self.annotations = {ann['image_id']: [] for ann in self.coco_data['annotations']}\n",
        "\n",
        "\n",
        "\n",
        "        #for ann in self.coco_data['annotations']:\n",
        "        #    self.annotations[ann['image_id']].append(ann)\n",
        "        self.annotations = {}\n",
        "        for ann in self.coco_data['annotations']:\n",
        "            #print(ann)\n",
        "            img_id = ann['image_id']\n",
        "            #print(img_id)\n",
        "            if img_id not in self.annotations:\n",
        "                self.annotations[img_id] = []\n",
        "            self.annotations[img_id].append(ann[\"bbox\"])\n",
        "\n",
        "\n",
        "\n",
        "        # Filter out missing files\n",
        "        self.valid_images = []\n",
        "        for img_id, file_name in self.images.items():\n",
        "            img_path = os.path.join(self.root_dir, file_name)\n",
        "            #print(self.annotations[img_id])\n",
        "            if os.path.exists(img_path):\n",
        "                self.valid_images.append((img_id, img_path))\n",
        "\n",
        "\n",
        "\n",
        "            else:\n",
        "                print(f\"Skipping missing file: {img_path}\")\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, img_path = self.valid_images[idx]\n",
        "\n",
        "\n",
        "        #img_path = os.path.join(self.root_dir, self.images[img_id])\n",
        "        image = load_npy_image(img_path)\n",
        "\n",
        "        #image = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Get annotations for the image\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in self.annotations[img_id]:\n",
        "            #bbox = ann['bbox']\n",
        "            # COCO format: [x, y, width, height] -> [x_min, y_min, x_max, y_max]\n",
        "            x_min, y_min, width, height = ann\n",
        "            x_max, y_max = x_min + width, y_min + height\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "            labels.append(1)  # Single class: thermal bridge\n",
        "\n",
        "        #boxes = np.array(boxes)\n",
        "        #labels = np.array(labels)\n",
        "\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image)\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "lAyni55UGV7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "l14oE52lNUcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "import torchvision\n",
        "\n",
        "# Load pretrained weights\n",
        "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
        "\n",
        "# Modify classifier head for 2 classes (1 class + background)\n",
        "num_classes = 2\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "6EA40cpJxbex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dont run fix this\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.rpn import RPNHead\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "import torchvision\n",
        "\n",
        "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
        "# create an anchor_generator for the FPN\n",
        "# which by default has 5 outputs\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=([(8, 16, 32, 64) for _ in range(4)]),  # Include small anchor sizes based on your histogram\n",
        "    aspect_ratios=([(0.5, 1.0, 2.0) for _ in range(4)])\n",
        ")\n",
        "\n",
        "model.rpn.anchor_generator = anchor_generator\n",
        "\n",
        "# 256 because that's the number of features that FPN returns\n",
        "model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
        "num_classes = 2\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "kZ0C30_ObRZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Paths\n",
        "train_images_dir = \"/content/drive/MyDrive/TBBR/train/images/\"\n",
        "train_annotations_file = \"/content/drive/MyDrive/TBBR/train/Flug1_100-104Media_coco.json\"\n",
        "#test_images_dir = \"test/images/\"\n",
        "#test_annotations_file = \"test/Flug1_105Media_coco.json\"\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "train_dataset = ThermalBridgeDataset(train_images_dir, train_annotations_file)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Define optimizer\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-4)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "all_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "        steps += 1\n",
        "\n",
        "    avg_loss = epoch_loss / steps\n",
        "    all_losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch + 1}, Avg Loss: {avg_loss:.4f}\")\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "plt.plot(range(1, num_epochs + 1), all_losses, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I7hk3bsJch2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing and Prediction"
      ],
      "metadata": {
        "id": "bDYThkJNNYGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "xsuYuOSfgSa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"thermal_bridge_fasterrcnn_v2_normalization.pth\")"
      ],
      "metadata": {
        "id": "bPYVIG44h34i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "test_images_dir = \"/content/drive/MyDrive/TBBR/test/images/\"\n",
        "test_annotations_file = \"/content/drive/MyDrive/TBBR/test/Flug1_105Media_coco.json\"\n",
        "\n",
        "test_dataset = ThermalBridgeDataset(test_images_dir, test_annotations_file)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
      ],
      "metadata": {
        "id": "0yO-Vylexbkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "#model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Thermal bridge models/thermal_bridge_fasterrcnn_v2_20_difflr.pth\"))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yNXcFXnYhiU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction"
      ],
      "metadata": {
        "id": "Ww53Q3emEJsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict single image\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Choose index of the image in your dataset\n",
        "index = 15 # Change this to test other images\n",
        "\n",
        "# Get image (no need for the target during inference)\n",
        "image, _ = test_dataset[index]  # Assuming your dataset object is named `test_dataset`\n",
        "\n",
        "# Add batch dimension and move to device\n",
        "image_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor)[0]\n"
      ],
      "metadata": {
        "id": "VqE2dS2f0ZLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert tensor to NumPy array for display\n",
        "img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "plt.imshow(img_np, cmap='gray')  # Use 'gray' if single-channel\n",
        "\n",
        "# Draw bounding boxes\n",
        "for box in output['boxes'].cpu().numpy():\n",
        "    x1, y1, x2, y2 = box.astype(int)\n",
        "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                      fill=False, edgecolor='red', linewidth=2))\n",
        "\n",
        "plt.title(\"Detected Boxes\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RE39NICp0Z9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def display_comparison(model, dataset, index, device, score_threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    # Load image and ground truth\n",
        "    image, target = dataset[index]\n",
        "    image_tensor = image.unsqueeze(0).to(device)\n",
        "    target = {k: v.to(device) for k, v in target.items()}\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)[0]\n",
        "\n",
        "    # Convert image to NumPy\n",
        "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Prepare figure\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # 1. Original Image\n",
        "    axs[0].imshow(img_np, cmap='gray')\n",
        "    axs[0].set_title(\"Original Image\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    # 2. Ground Truth\n",
        "    axs[1].imshow(img_np, cmap='gray')\n",
        "    for box in target['boxes'].cpu().numpy():\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        axs[1].add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                           linewidth=2, edgecolor='green', facecolor='none'))\n",
        "\n",
        "    axs[1].set_title(\"Ground Truth\")\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    # 3. Predictions\n",
        "    axs[2].imshow(img_np, cmap='gray')\n",
        "    for box, score in zip(output['boxes'].cpu().numpy(), output['scores'].cpu().numpy()):\n",
        "        if score >= score_threshold:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            axs[2].add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                               linewidth=2, edgecolor='red', facecolor='none'))\n",
        "            axs[2].text(x1, y1 - 5, f\"{score:.2f}\", color='red', fontsize=8)\n",
        "\n",
        "    axs[2].set_title(\"Predictions\")\n",
        "    axs[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "b8AEu0AD0aM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume model and dataset are ready\n",
        "display_comparison(model, test_dataset, index=67, device=device, score_threshold=0.1)\n"
      ],
      "metadata": {
        "id": "G9ndBRoT0i9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def display_overlay(model, dataset, index, device, score_threshold):\n",
        "    model.eval()\n",
        "\n",
        "    # Load image and ground truth\n",
        "    image, target = dataset[index]\n",
        "    image_tensor = image.unsqueeze(0).to(device)\n",
        "    target = {k: v.to(device) for k, v in target.items()}\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)[0]\n",
        "\n",
        "    # Convert image to NumPy\n",
        "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Prepare figure\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img_np)\n",
        "    ax = plt.gca()\n",
        "\n",
        "    # Plot ground truth boxes and masks in green\n",
        "    for box in target['boxes'].cpu().numpy():\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        ax.add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                       linewidth=2, edgecolor='green', facecolor='none'))\n",
        "\n",
        "\n",
        "    # Plot predicted boxes and masks in red\n",
        "    for box, score in zip(output['boxes'].cpu().numpy(), output['scores'].cpu().numpy()):\n",
        "        if score >= score_threshold:\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            ax.add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                           linewidth=2, edgecolor='red', facecolor='none'))\n",
        "            ax.text(x1, y1 - 5, f\"{score:.2f}\", color='red', fontsize=8)\n",
        "\n",
        "\n",
        "    plt.title(\"Overlay: Ground Truth (Green) vs Predictions (Red)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "th7q7Ku7FM9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_overlay(model, test_dataset, index=24, device=device, score_threshold=0.1)"
      ],
      "metadata": {
        "id": "2vy5SanHFPpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops import box_iou\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, dataset, device, iou_threshold=0.5, score_threshold=0.4):\n",
        "    model.eval()\n",
        "    aps, all_precisions, all_recalls, all_accuracies = [], [], [], []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        image, target = dataset[i]\n",
        "        image_tensor = image.to(device).unsqueeze(0)\n",
        "        target = {k: v.to(device) for k, v in target.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image_tensor)[0]\n",
        "\n",
        "        pred_boxes = output['boxes'][output['scores'] > score_threshold]\n",
        "        gt_boxes = target['boxes']\n",
        "\n",
        "        # If no predictions or GTs\n",
        "        if len(pred_boxes) == 0 and len(gt_boxes) == 0:\n",
        "            precision = recall = accuracy = ap = 1.0\n",
        "            tp = fp = fn = 0\n",
        "        elif len(pred_boxes) == 0:\n",
        "            tp = 0\n",
        "            fp = 0\n",
        "            fn = len(gt_boxes)\n",
        "            precision = recall = accuracy = ap = 0.0\n",
        "        elif len(gt_boxes) == 0:\n",
        "            tp = 0\n",
        "            fp = len(pred_boxes)\n",
        "            fn = 0\n",
        "            precision = recall = accuracy = ap = 0.0\n",
        "        else:\n",
        "            ious = box_iou(pred_boxes, gt_boxes)\n",
        "            matched = (ious > iou_threshold).any(dim=1).cpu().numpy()\n",
        "            tp = matched.sum()\n",
        "            fp = len(pred_boxes) - tp\n",
        "            fn = len(gt_boxes) - tp\n",
        "\n",
        "            precision = tp / (tp + fp + 1e-6)\n",
        "            recall = tp / (tp + fn + 1e-6)\n",
        "            accuracy = tp / (tp + fp + fn + 1e-6)\n",
        "            ap = precision * recall  # crude AP approximation\n",
        "\n",
        "        all_precisions.append(precision)\n",
        "        all_recalls.append(recall)\n",
        "        all_accuracies.append(accuracy)\n",
        "        aps.append(ap)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n==== Overall Metrics ====\")\n",
        "    print(f\"Mean Precision: {np.mean(all_precisions):.4f}\")\n",
        "    print(f\"Mean Recall:    {np.mean(all_recalls):.4f}\")\n",
        "    print(f\"Mean Accuracy:  {np.mean(all_accuracies):.4f}\")\n",
        "    print(f\"mAP (approx):   {np.mean(aps):.4f}\")\n"
      ],
      "metadata": {
        "id": "JECtZKmUF01c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model1(model, test_dataset, device=device)"
      ],
      "metadata": {
        "id": "IpNEgYIrF1u6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}